{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/nadirfartas/pfenotebook?scriptVersionId=236321355\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:55:59.385133Z","iopub.execute_input":"2025-05-02T10:55:59.385682Z","iopub.status.idle":"2025-05-02T10:55:59.390129Z","shell.execute_reply.started":"2025-05-02T10:55:59.385660Z","shell.execute_reply":"2025-05-02T10:55:59.389284Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nsecret_label = \"HF Hub\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\nlogin(token=secret_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:06:37.599301Z","iopub.execute_input":"2025-05-02T11:06:37.599729Z","iopub.status.idle":"2025-05-02T11:06:37.817105Z","shell.execute_reply.started":"2025-05-02T11:06:37.599705Z","shell.execute_reply":"2025-05-02T11:06:37.816585Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!pip install --upgrade pip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:06:41.059865Z","iopub.execute_input":"2025-05-02T11:06:41.060405Z","iopub.status.idle":"2025-05-02T11:06:47.832566Z","shell.execute_reply.started":"2025-05-02T11:06:41.060379Z","shell.execute_reply":"2025-05-02T11:06:47.831617Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.1-py3-none-any.whl.metadata (3.6 kB)\nDownloading pip-25.1-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.1\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install \\\n    datasets \\\n    evaluate \\\n    rouge_score\\\n    loralib \\\n    evaluate \\\n    accelerate \\\n    bitsandbytes \\\n    trl \\\n    peft \\\n    -U --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:16:48.570487Z","iopub.execute_input":"2025-05-02T11:16:48.570820Z","iopub.status.idle":"2025-05-02T11:18:00.407922Z","shell.execute_reply.started":"2025-05-02T11:16:48.570779Z","shell.execute_reply":"2025-05-02T11:18:00.407055Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m163.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[33m  DEPRECATION: Building 'rouge_score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge_score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n\u001b[0m  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16/16\u001b[0m [bitsandbytes][0m [bitsandbytes]er-cu12]\n\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import transformers\nimport torch\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:18:00.409499Z","iopub.execute_input":"2025-05-02T11:18:00.409793Z","iopub.status.idle":"2025-05-02T11:18:04.252339Z","shell.execute_reply.started":"2025-05-02T11:18:00.409766Z","shell.execute_reply":"2025-05-02T11:18:04.251776Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def format_squad_dataset(dataset_split):\n    \"\"\"\n    Format a SQuAD-style dataset split into a list of dictionaries \n    with context, questions, and answers.\n\n    Args:\n        dataset_split: a split of the SQuAD-style dataset, e.g., ds[\"train\"] or ds[\"validation\"]\n\n    Returns:\n        A list of formatted entries: [{\"context\": ..., \"questions\": [...], \"answers\": [...]}]\n    \"\"\"\n    formatted = []\n\n    for article in dataset_split:\n        for paragraph in article[\"data\"]:\n            for p in paragraph[\"paragraphs\"]:\n                context = p[\"context\"]\n                questions = []\n                answers = []\n\n                for qa in p[\"qas\"]:\n                    if qa[\"is_impossible\"]:\n                        continue\n\n                    if not qa.get(\"answers\") or not qa[\"answers\"][0].get(\"text\"):\n                        continue\n\n                    question = qa[\"question\"]\n                    answer = qa[\"answers\"][0][\"text\"]\n\n                    questions.append(question)\n                    answers.append(answer)\n\n                if questions and answers:\n                    formatted.append({\n                        \"context\": context,\n                        \"questions\": questions,\n                        \"answers\": answers\n                    })\n\n    return formatted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:18:04.253048Z","iopub.execute_input":"2025-05-02T11:18:04.253441Z","iopub.status.idle":"2025-05-02T11:18:04.259243Z","shell.execute_reply.started":"2025-05-02T11:18:04.253413Z","shell.execute_reply":"2025-05-02T11:18:04.258483Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from datasets import load_dataset\nimport json\n\n# Load dataset\nds = load_dataset(\"ZeyadAhmed/Arabic-SQuADv2.0\")\nformatted_train_dataset = format_squad_dataset(ds[\"train\"])\nformatted_val_dataset = format_squad_dataset(ds[\"validation\"])\n\nprint(f\"âœ… Done. Formatted {len(formatted_train_dataset)} training context blocks.\")\nprint(f\"âœ… Done. Formatted {len(formatted_val_dataset)} validation context blocks.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:18:04.260997Z","iopub.execute_input":"2025-05-02T11:18:04.261183Z","iopub.status.idle":"2025-05-02T11:18:14.953551Z","shell.execute_reply.started":"2025-05-02T11:18:04.261168Z","shell.execute_reply":"2025-05-02T11:18:14.952826Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"asquadv2-train.json:   0%|          | 0.00/91.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d56093e552541759193672c0db94f32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"asquadv2-val.json:   0%|          | 0.00/27.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e409aab3ce64d2195892053b113e3ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"asquadv2-test.json:   0%|          | 0.00/27.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c662b8a291dd4f9daece6e5bb4c77b58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b148e4848dea4bf689bf9e0355cb2048"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bc7a5b6bd044f66a6034eafc5d0c28d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f808f446d4f4451384553c69deb03847"}},"metadata":{}},{"name":"stdout","text":"âœ… Done. Formatted 17180 training context blocks.\nâœ… Done. Formatted 4639 validation context blocks.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def flatten_context_qa_pairs(formatted_data):\n    \"\"\"\n    Convert formatted context-question-answer data into a flat list of\n    input-target training pairs for seq2seq modeling.\n\n    Args:\n        formatted_data: List of dicts with 'context', 'questions', and 'answers'\n\n    Returns:\n        A list of dicts with 'input' and 'target' fields\n    \"\"\"\n    flat_data = []\n\n    for item in formatted_data:\n        context = item[\"context\"]\n        for question, answer in zip(item[\"questions\"], item[\"answers\"]):\n            input_text = f\"context: {context} answer: {answer}\"\n            target_text = question\n            flat_data.append({\"input\": input_text, \"target\": target_text})\n\n    return flat_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:18:14.954333Z","iopub.execute_input":"2025-05-02T11:18:14.954838Z","iopub.status.idle":"2025-05-02T11:18:14.959369Z","shell.execute_reply.started":"2025-05-02T11:18:14.954788Z","shell.execute_reply":"2025-05-02T11:18:14.958603Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"train_dataset = flatten_context_qa_pairs(formatted_train_dataset)\nval_dataset = flatten_context_qa_pairs(formatted_val_dataset)\nprint(f\"Total training pairs: {len(train_dataset)}\")\nprint(f\"Total validation pairs: {len(val_dataset)}\")\nprint(train_dataset[1])\nprint(val_dataset[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:18:14.960313Z","iopub.execute_input":"2025-05-02T11:18:14.960553Z","iopub.status.idle":"2025-05-02T11:18:15.478915Z","shell.execute_reply.started":"2025-05-02T11:18:14.960538Z","shell.execute_reply":"2025-05-02T11:18:15.478126Z"}},"outputs":[{"name":"stdout","text":"Total training pairs: 42033\nTotal validation pairs: 5254\n{'input': 'context: ( Ù„Ù… ÙŠÙƒÙ† Ø²Ù„Ø²Ø§Ù„ Ms 6 . 1 ÙÙŠ 30 Ø£ØºØ³Ø·Ø³ 2008 ÙÙŠ Ø¬Ù†ÙˆØ¨ Ø³ÙŠØªØ´ÙˆØ§Ù† Ø¬Ø²Ø¡Ø§ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ù„Ø£Ù†Ù‡ ÙƒØ§Ù† Ù†Ø§Ø¬Ù…Ø§ Ø¹Ù† ØµØ¯Ø¹ Ù…Ø®ØªÙ„Ù . Ø§Ù†Ø¸Ø± Ø²Ù„Ø²Ø§Ù„ Ø¨Ø§Ù†ØªØ´ÙŠÙ‡ÙˆØ§ 2008 Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§ØµÙŠÙ„ . ) answer: Ø²Ù„Ø²Ø§Ù„ Ø¨Ø§Ù†ØªØ´ÙŠÙ‡ÙˆØ§ 2008', 'target': 'Ø£ÙŠÙ† ÙŠØ¬Ø¨ Ø£Ù† ØªØ¨Ø­Ø« Ø¹Ù† Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ ØŸ'}\n{'input': 'context: ØºØ§Ø¯Ø± ÙØ±ÙŠÙ‚ Ø§Ù„Ø¥ØºØ§Ø«Ø© Ø§Ù„Ø·Ø§Ø±Ø¦Ø© Ù…Ù† Ø§Ù„Ø²Ù„Ø§Ø²Ù„ Ø§Ù„Ù…ÙƒÙˆÙ† Ù…Ù† 184 Ø´Ø®ØµØ§ ( ÙŠØªØ£Ù„Ù Ù…Ù† 12 Ø´Ø®ØµØ§ Ù…Ù† Ù…ÙƒØªØ¨ Ø§Ù„Ø¯ÙˆÙ„Ø© Ù„Ø±ØµØ¯ Ø§Ù„Ø²Ù„Ø§Ø²Ù„ Ùˆ 150 Ù…Ù† Ù‚ÙŠØ§Ø¯Ø© Ù…Ù†Ø·Ù‚Ø© Ø¨ÙƒÙŠÙ† Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠØ© Ùˆ 22 Ù…Ù† Ù…Ø³ØªØ´ÙÙ‰ Ø§Ù„Ø´Ø±Ø·Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ø§Ù„Ù…Ø³Ù„Ø­Ø© ) Ø¨ÙƒÙŠÙ† Ù…Ù† Ù…Ø·Ø§Ø± Ù†Ø§Ù†ÙŠÙˆØ§Ù† ÙÙŠ Ø£ÙˆØ§Ø®Ø± 12 Ù…Ø§ÙŠÙˆ ÙÙŠ Ø·Ø§Ø¦Ø±ØªÙŠ Ù†Ù‚Ù„ Ø¹Ø³ÙƒØ±ÙŠØªÙŠÙ† Ù„Ù„Ø³ÙØ± Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹Ø© ÙˆÙ†ØªØ´ÙˆØ§Ù† . answer: 184', 'target': 'ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠÙ† Ø´ÙƒÙ„ÙˆØ§ ÙØ±ÙŠÙ‚ Ø§Ù„Ø¥ØºØ§Ø«Ø© ØŸ'}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\nfrom datasets import Dataset\n\n# Load tokenizer\ntokenizer = T5Tokenizer.from_pretrained(\"UBC-NLP/AraT5v2-base-1024\", legacy=False)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"UBC-NLP/AraT5v2-base-1024\")\n# Convert list to Hugging Face Dataset\nhf_train_dataset = Dataset.from_list(train_dataset)\nhf_val_dataset = Dataset.from_list(val_dataset)\n\n# Tokenize function\ndef tokenize(example):\n    model_input = tokenizer(\n        example[\"input\"], \n        max_length=512, \n        padding=\"max_length\", \n        truncation=True\n    )\n    labels = tokenizer(\n        example[\"target\"], \n        max_length=64, \n        padding=\"max_length\", \n        truncation=True\n    )\n    model_input[\"labels\"] = labels[\"input_ids\"]\n    return model_input\n\n# Apply tokenizer\ntokenized_train_dataset = hf_train_dataset.map(tokenize, batched=False)\ntokenized_val_dataset = hf_val_dataset.map(tokenize, batched=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:19:29.772013Z","iopub.execute_input":"2025-05-02T11:19:29.772687Z","iopub.status.idle":"2025-05-02T11:20:56.780295Z","shell.execute_reply.started":"2025-05-02T11:19:29.772663Z","shell.execute_reply":"2025-05-02T11:20:56.779598Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/42033 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"057a65c5d0734d24888ae3f0b316542b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5254 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09fea35bd1a9447a96cc5dabce501f2d"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"import torch\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:22:18.448549Z","iopub.execute_input":"2025-05-02T11:22:18.448891Z","iopub.status.idle":"2025-05-02T11:22:19.116484Z","shell.execute_reply.started":"2025-05-02T11:22:18.448859Z","shell.execute_reply":"2025-05-02T11:22:19.115655Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nDevice: cuda\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./araT5-qg\",\n    run_name=\"araT5-qg-run-1\",\n    learning_rate=2e-5,                           # standard LR for T5 fine-tuning\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n\n    logging_dir=\"./logs\",\n    logging_strategy=\"steps\",\n    logging_steps=100,\n    save_total_limit=1,                           # keep only 2 latest checkpoints\n    eval_strategy=\"epoch\",           # use eval_strategy (not evaluation_strategy)\n    save_strategy=\"epoch\",\n    fp16=True, \n    metric_for_best_model=\"loss\",\n    greater_is_better=False,\n    load_best_model_at_end=False,\n    report_to=\"none\",                             # no wandb/huggingface reporting\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_val_dataset,\n    tokenizer=tokenizer,\n)\n\n# Start training\ntrainer.train()\n\n# Save final model and tokenizer\nmodel.save_pretrained(\"./araT5-qg-final\")\ntokenizer.save_pretrained(\"./araT5-qg-final\")\n\nprint(\"âœ… Training complete! Model and tokenizer saved to './araT5-qg-final'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T11:22:26.799862Z","iopub.execute_input":"2025-05-02T11:22:26.800148Z","iopub.status.idle":"2025-05-02T15:12:41.399851Z","shell.execute_reply.started":"2025-05-02T11:22:26.800126Z","shell.execute_reply":"2025-05-02T15:12:41.398989Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2099068984.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7884' max='7884' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7884/7884 3:50:06, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.486000</td>\n      <td>0.391099</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.453800</td>\n      <td>0.377061</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.445300</td>\n      <td>0.373185</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"âœ… Training complete! Model and tokenizer saved to './araT5-qg-final'\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!pip install huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:16:22.851495Z","iopub.execute_input":"2025-05-02T15:16:22.852219Z","iopub.status.idle":"2025-05-02T15:16:25.227444Z","shell.execute_reply.started":"2025-05-02T15:16:22.852196Z","shell.execute_reply":"2025-05-02T15:16:25.226448Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Go to the saved model directory\n%cd /kaggle/working/araT5-qg-final\n\n# Initialize Git repo (if not already a repo)\n!git init\n\n# Configure your identity\n!git config user.email \"fartasnadir2003@gmail.com\"\n!git config user.name \"NadirFartas\"\n\n# Link to your Hugging Face repo\n!git remote add origin https://huggingface.co/NadirFartas/araT5-qg-final\n\n# Add and commit files\n!git add .\n!git commit -m \"Initial commit of AraT5 fine-tuned model\"\n\n# Set the branch to main\n!git branch -M main\n\n# Push to Hugging Face\n!git push origin main\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:51:59.609988Z","iopub.execute_input":"2025-05-02T15:51:59.610294Z","iopub.status.idle":"2025-05-02T15:52:11.176094Z","shell.execute_reply.started":"2025-05-02T15:51:59.610270Z","shell.execute_reply":"2025-05-02T15:52:11.175060Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\nReinitialized existing Git repository in /kaggle/working/araT5-qg-final/.git/\nerror: remote origin already exists.\nOn branch main\nnothing to commit, working tree clean\nUsername for 'https://huggingface.co': ^C\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# Go to the saved model directory\n%cd /kaggle/working/araT5-qg-final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:43:23.397591Z","iopub.execute_input":"2025-05-02T15:43:23.397951Z","iopub.status.idle":"2025-05-02T15:43:23.403264Z","shell.execute_reply.started":"2025-05-02T15:43:23.397920Z","shell.execute_reply":"2025-05-02T15:43:23.402459Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Initialize Git repo (if not already a repo)\n!git init","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:43:33.119502Z","iopub.execute_input":"2025-05-02T15:43:33.119969Z","iopub.status.idle":"2025-05-02T15:43:33.344388Z","shell.execute_reply.started":"2025-05-02T15:43:33.119944Z","shell.execute_reply":"2025-05-02T15:43:33.343606Z"}},"outputs":[{"name":"stdout","text":"Reinitialized existing Git repository in /kaggle/working/araT5-qg-final/.git/\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"!git config user.email \"fartasnadir2003@gmail.com\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:43:47.821547Z","iopub.execute_input":"2025-05-02T15:43:47.822222Z","iopub.status.idle":"2025-05-02T15:43:48.049607Z","shell.execute_reply.started":"2025-05-02T15:43:47.822194Z","shell.execute_reply":"2025-05-02T15:43:48.048535Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"!git config user.name \"NadirFartas\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:44:00.975924Z","iopub.execute_input":"2025-05-02T15:44:00.976215Z","iopub.status.idle":"2025-05-02T15:44:01.201142Z","shell.execute_reply.started":"2025-05-02T15:44:00.976192Z","shell.execute_reply":"2025-05-02T15:44:01.200188Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Add and commit files\n!git add .\n!git commit -m \"Initial commit of AraT5 fine-tuned model\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:09:34.977890Z","iopub.execute_input":"2025-05-02T16:09:34.978569Z","iopub.status.idle":"2025-05-02T16:09:35.434595Z","shell.execute_reply.started":"2025-05-02T16:09:34.978539Z","shell.execute_reply":"2025-05-02T16:09:35.433850Z"}},"outputs":[{"name":"stdout","text":"On branch main\nnothing to commit, working tree clean\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:48:21.870599Z","iopub.execute_input":"2025-05-02T15:48:21.870938Z","iopub.status.idle":"2025-05-02T15:48:22.106783Z","shell.execute_reply.started":"2025-05-02T15:48:21.870907Z","shell.execute_reply":"2025-05-02T15:48:22.105767Z"}},"outputs":[{"name":"stdout","text":"added_tokens.json\tmodel.safetensors\t spiece.model\nconfig.json\t\tREADME.md\t\t tokenizer_config.json\ngeneration_config.json\tspecial_tokens_map.json\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"!git branch -M main","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:09:44.515535Z","iopub.execute_input":"2025-05-02T16:09:44.516193Z","iopub.status.idle":"2025-05-02T16:09:44.741636Z","shell.execute_reply.started":"2025-05-02T16:09:44.516167Z","shell.execute_reply":"2025-05-02T16:09:44.740781Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"!git remote set-url origin https://NadirFartas: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@huggingface.co/NadirFartas/araT5-qg-final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:08:57.600136Z","iopub.execute_input":"2025-05-02T16:08:57.600644Z","iopub.status.idle":"2025-05-02T16:08:57.829709Z","shell.execute_reply.started":"2025-05-02T16:08:57.600620Z","shell.execute_reply":"2025-05-02T16:08:57.828648Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"!git push origin main","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:10:04.230279Z","iopub.execute_input":"2025-05-02T16:10:04.231028Z","iopub.status.idle":"2025-05-02T16:10:20.381245Z","shell.execute_reply.started":"2025-05-02T16:10:04.230982Z","shell.execute_reply":"2025-05-02T16:10:20.380133Z"}},"outputs":[{"name":"stdout","text":"Uploading LFS objects: 100% (2/2), 1.5 GB | 102 MB/s, done.                     \nEnumerating objects: 10, done.\nCounting objects: 100% (10/10), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (9/9), done.\nWriting objects: 100% (9/9), 3.22 KiB | 1.61 MiB/s, done.\nTotal 9 (delta 0), reused 0 (delta 0), pack-reused 0\nTo https://huggingface.co/NadirFartas/araT5-qg-final\n   10f9fa8..3995e3f  main -> main\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"NadirFartas/araT5-qg-final\")\ntokenizer = AutoTokenizer.from_pretrained(\"NadirFartas/araT5-qg-final\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:14:48.804365Z","iopub.execute_input":"2025-05-02T16:14:48.805079Z","iopub.status.idle":"2025-05-02T16:15:17.711730Z","shell.execute_reply.started":"2025-05-02T16:14:48.805050Z","shell.execute_reply":"2025-05-02T16:15:17.711157Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/787 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05954309b7dc4616a8450e5a2ad1ccc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df769bdad26c43e5941db21a8f2e1aed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c43ca0eb99594aff940c3bc35f4ffd28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40e53e90eb3c44e989a85f9f441b796e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/2.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3e2665dbdf24372a373da5253862d1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7658218944b445c9b3d9193ddb556f52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f82cdfb251c4fdb852123117ab18749"}},"metadata":{}},{"name":"stderr","text":"You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"print(ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:22:11.697846Z","iopub.execute_input":"2025-05-02T16:22:11.698438Z","iopub.status.idle":"2025-05-02T16:22:11.702530Z","shell.execute_reply.started":"2025-05-02T16:22:11.698415Z","shell.execute_reply":"2025-05-02T16:22:11.701777Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['version', 'data'],\n        num_rows: 1\n    })\n    validation: Dataset({\n        features: ['version', 'data'],\n        num_rows: 1\n    })\n    test: Dataset({\n        features: ['version', 'data'],\n        num_rows: 1\n    })\n})\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"formatted_test_dataset = format_squad_dataset(ds[\"test\"])\ntest_dataset = flatten_context_qa_pairs(formatted_test_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:24:59.101674Z","iopub.execute_input":"2025-05-02T16:24:59.102536Z","iopub.status.idle":"2025-05-02T16:24:59.911794Z","shell.execute_reply.started":"2025-05-02T16:24:59.102499Z","shell.execute_reply":"2025-05-02T16:24:59.910958Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"print(f\"Total test pairs: {len(test_dataset)}\")\nprint(test_dataset[3])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:26:52.296925Z","iopub.execute_input":"2025-05-02T16:26:52.297477Z","iopub.status.idle":"2025-05-02T16:26:52.301426Z","shell.execute_reply.started":"2025-05-02T16:26:52.297453Z","shell.execute_reply":"2025-05-02T16:26:52.300613Z"}},"outputs":[{"name":"stdout","text":"Total test pairs: 5254\n{'input': 'context: Ø¨Ø­Ù„ÙˆÙ„ 15 Ù…Ø§ÙŠÙˆ ØŒ Ø£Ù…Ø± Ø±Ø¦ÙŠØ³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¯ÙˆÙ„Ø© ÙˆÙ† Ø¬ÙŠØ§Ø¨Ø§Ùˆ Ø¨Ù†Ø´Ø± 90 Ø·Ø§Ø¦Ø±Ø© Ù‡Ù„ÙŠÙƒÙˆØ¨ØªØ± Ø¥Ø¶Ø§ÙÙŠØ© ØŒ Ù…Ù†Ù‡Ø§ 60 Ø·Ø§Ø¦Ø±Ø© Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Ù‚Ø¨Ù„ Ø¬ÙŠØ´ Ø§Ù„ØªØ­Ø±ÙŠØ± Ø§Ù„Ø´Ø¹Ø¨ÙŠ Ø§Ù„ØµÙŠÙ†ÙŠ ØŒ Ùˆ 30 Ø·Ø§Ø¦Ø±Ø© Ù…Ù† Ø§Ù„Ù…Ù‚Ø±Ø± Ø£Ù† ØªÙˆÙØ±Ù‡Ø§ ØµÙ†Ø§Ø¹Ø© Ø§Ù„Ø·ÙŠØ±Ø§Ù† Ø§Ù„Ù…Ø¯Ù†ÙŠ ØŒ Ù„ÙŠØµÙ„ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø§Ø¦Ø±Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… Ù†Ø´Ø±Ù‡Ø§ ÙÙŠ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¥ØºØ§Ø«Ø© Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ù‚ÙˆØ§Øª Ø§Ù„Ø¬ÙˆÙŠØ© ÙˆØ§Ù„Ø¬ÙŠØ´ ÙˆØ§Ù„Ø·ÙŠØ±Ø§Ù† Ø§Ù„Ù…Ø¯Ù†ÙŠ Ø¥Ù„Ù‰ Ø£ÙƒØ«Ø± Ù…Ù† 150 ØŒ Ù…Ù…Ø§ Ø£Ø¯Ù‰ Ø¥Ù„Ù‰ Ø£ÙƒØ¨Ø± Ø¹Ù…Ù„ÙŠØ© Ù†Ù‚Ù„ Ø¬ÙˆÙŠ ØºÙŠØ± Ù‚ØªØ§Ù„ÙŠØ© ÙÙŠ ØªØ§Ø±ÙŠØ® Ø¬ÙŠØ´ Ø§Ù„ØªØ­Ø±ÙŠØ± Ø§Ù„Ø´Ø¹Ø¨ÙŠ . answer: Ø£ÙƒØ«Ø± Ù…Ù† 150', 'target': 'ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø§Ø¦Ø±Ø§Øª Ø§Ù„ØªÙŠ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ ØŸ'}\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"import torch\n\ndef generate_question_from_input(input_text, max_length=64):\n    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True).to(model.device)\n    outputs = model.generate(inputs, max_length=max_length, num_beams=4, early_stopping=True)\n    question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return question\n\n# Pick a test example\nsample = test_dataset[3]\ninput_text = sample[\"input\"]\nreal_question = sample[\"target\"]\n\n# Generate question\ngenerated_question = generate_question_from_input(input_text)\n\n# Show results\nprint(\"ğŸ“¥ Input to the model:\\n\", input_text)\nprint(\"\\nâœ… Real Question:\\n\", real_question)\nprint(\"\\nğŸ¤– Generated Question:\\n\", generated_question)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T17:05:19.346272Z","iopub.execute_input":"2025-05-02T17:05:19.346535Z","iopub.status.idle":"2025-05-02T17:05:21.147646Z","shell.execute_reply.started":"2025-05-02T17:05:19.346515Z","shell.execute_reply":"2025-05-02T17:05:21.146861Z"}},"outputs":[{"name":"stdout","text":"ğŸ“¥ Input to the model:\n context: Ø¨Ø­Ù„ÙˆÙ„ 15 Ù…Ø§ÙŠÙˆ ØŒ Ø£Ù…Ø± Ø±Ø¦ÙŠØ³ Ù…Ø¬Ù„Ø³ Ø§Ù„Ø¯ÙˆÙ„Ø© ÙˆÙ† Ø¬ÙŠØ§Ø¨Ø§Ùˆ Ø¨Ù†Ø´Ø± 90 Ø·Ø§Ø¦Ø±Ø© Ù‡Ù„ÙŠÙƒÙˆØ¨ØªØ± Ø¥Ø¶Ø§ÙÙŠØ© ØŒ Ù…Ù†Ù‡Ø§ 60 Ø·Ø§Ø¦Ø±Ø© Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Ù‚Ø¨Ù„ Ø¬ÙŠØ´ Ø§Ù„ØªØ­Ø±ÙŠØ± Ø§Ù„Ø´Ø¹Ø¨ÙŠ Ø§Ù„ØµÙŠÙ†ÙŠ ØŒ Ùˆ 30 Ø·Ø§Ø¦Ø±Ø© Ù…Ù† Ø§Ù„Ù…Ù‚Ø±Ø± Ø£Ù† ØªÙˆÙØ±Ù‡Ø§ ØµÙ†Ø§Ø¹Ø© Ø§Ù„Ø·ÙŠØ±Ø§Ù† Ø§Ù„Ù…Ø¯Ù†ÙŠ ØŒ Ù„ÙŠØµÙ„ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø§Ø¦Ø±Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… Ù†Ø´Ø±Ù‡Ø§ ÙÙŠ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¥ØºØ§Ø«Ø© Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ù‚ÙˆØ§Øª Ø§Ù„Ø¬ÙˆÙŠØ© ÙˆØ§Ù„Ø¬ÙŠØ´ ÙˆØ§Ù„Ø·ÙŠØ±Ø§Ù† Ø§Ù„Ù…Ø¯Ù†ÙŠ Ø¥Ù„Ù‰ Ø£ÙƒØ«Ø± Ù…Ù† 150 ØŒ Ù…Ù…Ø§ Ø£Ø¯Ù‰ Ø¥Ù„Ù‰ Ø£ÙƒØ¨Ø± Ø¹Ù…Ù„ÙŠØ© Ù†Ù‚Ù„ Ø¬ÙˆÙŠ ØºÙŠØ± Ù‚ØªØ§Ù„ÙŠØ© ÙÙŠ ØªØ§Ø±ÙŠØ® Ø¬ÙŠØ´ Ø§Ù„ØªØ­Ø±ÙŠØ± Ø§Ù„Ø´Ø¹Ø¨ÙŠ . answer: Ø£ÙƒØ«Ø± Ù…Ù† 150\n\nâœ… Real Question:\n ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø§Ø¦Ø±Ø§Øª Ø§Ù„ØªÙŠ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ ØŸ\n\nğŸ¤– Generated Question:\n ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø§Ø¦Ø±Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… Ù†Ø´Ø±Ù‡Ø§ ÙÙŠ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¥ØºØ§Ø«Ø© ØŸ\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load(\"rouge\")\nbleu = evaluate.load(\"bleu\")\npredictions = []\nreferences = []\n\nfor sample in test_dataset[:500]:  # evaluate on a subset to be faster\n    input_text = sample[\"input\"]\n    reference = sample[\"target\"]\n    \n    # Generate question\n    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True).to(model.device)\n    outputs = model.generate(inputs, max_length=64, num_beams=4)\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    predictions.append(generated)\n    references.append(reference)\n\n# ROUGE expects dict with lists\nrouge_result = rouge.compute(predictions=predictions, references=references)\nprint(\"ğŸ” ROUGE scores:\")\nprint(rouge_result)\n\n# BLEU expects references as list of lists\nbleu_result = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\nprint(\"\\nğŸ” BLEU score:\")\nprint(bleu_result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T17:07:21.542043Z","iopub.execute_input":"2025-05-02T17:07:21.542300Z","iopub.status.idle":"2025-05-02T17:21:28.317893Z","shell.execute_reply.started":"2025-05-02T17:07:21.542284Z","shell.execute_reply":"2025-05-02T17:21:28.317108Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"000dce57b6ad43dca2df0137fa856765"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a58b06f455104dfaad04705c1abcfac4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfe45baa49dd4674b26553715801b27c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f86a14f1fab04f53b2b75cb1d6d579b0"}},"metadata":{}},{"name":"stdout","text":"ğŸ” ROUGE scores:\n{'rouge1': 0.08273333333333334, 'rouge2': 0.018166666666666664, 'rougeL': 0.08276666666666666, 'rougeLsum': 0.08233333333333331}\n\nğŸ” BLEU score:\n{'bleu': 0.17064651710073242, 'precisions': [0.4323908443931525, 0.20429878697595233, 0.12502976899261728, 0.07677750743444174], 'brevity_penalty': 1.0, 'length_ratio': 1.0186128526645768, 'translation_length': 5199, 'reference_length': 5104}\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"!pip install bert-score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}