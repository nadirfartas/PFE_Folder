{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/nadirfartas/pfenotebook?scriptVersionId=236321355\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T10:55:59.385133Z","iopub.execute_input":"2025-05-02T10:55:59.385682Z","iopub.status.idle":"2025-05-02T10:55:59.390129Z","shell.execute_reply.started":"2025-05-02T10:55:59.385660Z","shell.execute_reply":"2025-05-02T10:55:59.389284Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nsecret_label = \"HF Hub\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\nlogin(token=secret_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:17:19.667468Z","iopub.execute_input":"2025-05-02T03:17:19.667753Z","iopub.status.idle":"2025-05-02T03:17:20.766262Z","shell.execute_reply.started":"2025-05-02T03:17:19.667726Z","shell.execute_reply":"2025-05-02T03:17:20.765738Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install --upgrade pip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:17:20.767023Z","iopub.execute_input":"2025-05-02T03:17:20.767276Z","iopub.status.idle":"2025-05-02T03:17:28.679089Z","shell.execute_reply.started":"2025-05-02T03:17:20.767254Z","shell.execute_reply":"2025-05-02T03:17:28.678154Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.1-py3-none-any.whl.metadata (3.6 kB)\nDownloading pip-25.1-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install \\\n    datasets \\\n    evaluate \\\n    rouge_score\\\n    loralib \\\n    evaluate \\\n    accelerate \\\n    bitsandbytes \\\n    trl \\\n    peft \\\n    -U --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:17:28.680144Z","iopub.execute_input":"2025-05-02T03:17:28.680419Z","iopub.status.idle":"2025-05-02T03:19:00.923610Z","shell.execute_reply.started":"2025-05-02T03:17:28.680381Z","shell.execute_reply":"2025-05-02T03:19:00.922945Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[33m  DEPRECATION: Building 'rouge_score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge_score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n\u001b[0m  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [bitsandbytes][0m [bitsandbytes]er-cu12]\n\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import transformers\nimport torch\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:19:00.924585Z","iopub.execute_input":"2025-05-02T03:19:00.924872Z","iopub.status.idle":"2025-05-02T03:19:06.718944Z","shell.execute_reply.started":"2025-05-02T03:19:00.924835Z","shell.execute_reply":"2025-05-02T03:19:06.718099Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def format_squad_dataset(dataset_split):\n    \"\"\"\n    Format a SQuAD-style dataset split into a list of dictionaries \n    with context, questions, and answers.\n\n    Args:\n        dataset_split: a split of the SQuAD-style dataset, e.g., ds[\"train\"] or ds[\"validation\"]\n\n    Returns:\n        A list of formatted entries: [{\"context\": ..., \"questions\": [...], \"answers\": [...]}]\n    \"\"\"\n    formatted = []\n\n    for article in dataset_split:\n        for paragraph in article[\"data\"]:\n            for p in paragraph[\"paragraphs\"]:\n                context = p[\"context\"]\n                questions = []\n                answers = []\n\n                for qa in p[\"qas\"]:\n                    if qa[\"is_impossible\"]:\n                        continue\n\n                    if not qa.get(\"answers\") or not qa[\"answers\"][0].get(\"text\"):\n                        continue\n\n                    question = qa[\"question\"]\n                    answer = qa[\"answers\"][0][\"text\"]\n\n                    questions.append(question)\n                    answers.append(answer)\n\n                if questions and answers:\n                    formatted.append({\n                        \"context\": context,\n                        \"questions\": questions,\n                        \"answers\": answers\n                    })\n\n    return formatted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:19:06.721391Z","iopub.execute_input":"2025-05-02T03:19:06.721765Z","iopub.status.idle":"2025-05-02T03:19:06.727671Z","shell.execute_reply.started":"2025-05-02T03:19:06.721733Z","shell.execute_reply":"2025-05-02T03:19:06.726967Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from datasets import load_dataset\nimport json\n\n# Load dataset\nds = load_dataset(\"ZeyadAhmed/Arabic-SQuADv2.0\")\nformatted_train_dataset = format_squad_dataset(ds[\"train\"])\nformatted_val_dataset = format_squad_dataset(ds[\"validation\"])\n\nprint(f\"✅ Done. Formatted {len(formatted_train_dataset)} training context blocks.\")\nprint(f\"✅ Done. Formatted {len(formatted_val_dataset)} validation context blocks.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:19:06.728376Z","iopub.execute_input":"2025-05-02T03:19:06.728594Z","iopub.status.idle":"2025-05-02T03:19:19.348326Z","shell.execute_reply.started":"2025-05-02T03:19:06.728569Z","shell.execute_reply":"2025-05-02T03:19:19.347599Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"asquadv2-train.json:   0%|          | 0.00/91.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9ad52fe82e6425c8ffbfe96deaafb68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"asquadv2-val.json:   0%|          | 0.00/27.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"876525525e12457ca52cd504329db370"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"asquadv2-test.json:   0%|          | 0.00/27.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b43c6011098944e2af779bc270192b36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0f1d62b96474691b1fbc8169f7e96ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"427fdad247ed4b29acbd5139384af110"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c039e946c63b4a588c4fd5d38037e967"}},"metadata":{}},{"name":"stdout","text":"✅ Done. Formatted 17180 training context blocks.\n✅ Done. Formatted 4639 validation context blocks.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def flatten_context_qa_pairs(formatted_data):\n    \"\"\"\n    Convert formatted context-question-answer data into a flat list of\n    input-target training pairs for seq2seq modeling.\n\n    Args:\n        formatted_data: List of dicts with 'context', 'questions', and 'answers'\n\n    Returns:\n        A list of dicts with 'input' and 'target' fields\n    \"\"\"\n    flat_data = []\n\n    for item in formatted_data:\n        context = item[\"context\"]\n        for question, answer in zip(item[\"questions\"], item[\"answers\"]):\n            input_text = f\"context: {context} answer: {answer}\"\n            target_text = question\n            flat_data.append({\"input\": input_text, \"target\": target_text})\n\n    return flat_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:19:19.349070Z","iopub.execute_input":"2025-05-02T03:19:19.349465Z","iopub.status.idle":"2025-05-02T03:19:19.354190Z","shell.execute_reply.started":"2025-05-02T03:19:19.349423Z","shell.execute_reply":"2025-05-02T03:19:19.353651Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_dataset = flatten_context_qa_pairs(formatted_train_dataset)\nval_dataset = flatten_context_qa_pairs(formatted_val_dataset)\nprint(f\"Total training pairs: {len(train_dataset)}\")\nprint(f\"Total validation pairs: {len(val_dataset)}\")\nprint(train_dataset[1])\nprint(val_dataset[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:19:19.354912Z","iopub.execute_input":"2025-05-02T03:19:19.355112Z","iopub.status.idle":"2025-05-02T03:19:19.464140Z","shell.execute_reply.started":"2025-05-02T03:19:19.355089Z","shell.execute_reply":"2025-05-02T03:19:19.463379Z"}},"outputs":[{"name":"stdout","text":"Total training pairs: 42033\nTotal validation pairs: 5254\n{'input': 'context: ( لم يكن زلزال Ms 6 . 1 في 30 أغسطس 2008 في جنوب سيتشوان جزءا من هذه السلسلة لأنه كان ناجما عن صدع مختلف . انظر زلزال بانتشيهوا 2008 للحصول على التفاصيل . ) answer: زلزال بانتشيهوا 2008', 'target': 'أين يجب أن تبحث عن مزيد من التفاصيل ؟'}\n{'input': 'context: غادر فريق الإغاثة الطارئة من الزلازل المكون من 184 شخصا ( يتألف من 12 شخصا من مكتب الدولة لرصد الزلازل و 150 من قيادة منطقة بكين العسكرية و 22 من مستشفى الشرطة العامة المسلحة ) بكين من مطار نانيوان في أواخر 12 مايو في طائرتي نقل عسكريتين للسفر إلى مقاطعة ونتشوان . answer: 184', 'target': 'كم عدد الأشخاص الذين شكلوا فريق الإغاثة ؟'}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\nfrom datasets import Dataset\n\n# Load tokenizer\ntokenizer = T5Tokenizer.from_pretrained(\"UBC-NLP/AraT5v2-base-1024\", legacy=False)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"UBC-NLP/AraT5v2-base-1024\")\n# Convert list to Hugging Face Dataset\nhf_train_dataset = Dataset.from_list(train_dataset)\nhf_val_dataset = Dataset.from_list(val_dataset)\n\n# Tokenize function\ndef tokenize(example):\n    model_input = tokenizer(\n        example[\"input\"], \n        max_length=512, \n        padding=\"max_length\", \n        truncation=True\n    )\n    labels = tokenizer(\n        example[\"target\"], \n        max_length=64, \n        padding=\"max_length\", \n        truncation=True\n    )\n    model_input[\"labels\"] = labels[\"input_ids\"]\n    return model_input\n\n# Apply tokenizer\ntokenized_train_dataset = hf_train_dataset.map(tokenize, batched=False)\ntokenized_val_dataset = hf_val_dataset.map(tokenize, batched=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:22:33.295255Z","iopub.execute_input":"2025-05-02T03:22:33.295575Z","iopub.status.idle":"2025-05-02T03:23:58.488236Z","shell.execute_reply.started":"2025-05-02T03:22:33.295552Z","shell.execute_reply":"2025-05-02T03:23:58.487453Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/42033 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2a4222db7b04702b581ffd6667e268d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5254 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"314d9c02825547488fc3ec89a049de75"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"import torch\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:24:09.777338Z","iopub.execute_input":"2025-05-02T03:24:09.777635Z","iopub.status.idle":"2025-05-02T03:24:10.235748Z","shell.execute_reply.started":"2025-05-02T03:24:09.777613Z","shell.execute_reply":"2025-05-02T03:24:10.235160Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nDevice: cuda\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./araT5-qg\",\n    run_name=\"araT5-qg-run-1\",\n    learning_rate=2e-5,                           # standard LR for T5 fine-tuning\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n\n    logging_dir=\"./logs\",\n    logging_strategy=\"steps\",\n    logging_steps=100,\n    save_total_limit=1,                           # keep only 2 latest checkpoints\n    eval_strategy=\"epoch\",           # use eval_strategy (not evaluation_strategy)\n    save_strategy=\"epoch\",\n    fp16=True, \n    metric_for_best_model=\"loss\",\n    greater_is_better=False,\n    load_best_model_at_end=False,\n    report_to=\"none\",                             # no wandb/huggingface reporting\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_val_dataset,\n    tokenizer=tokenizer,\n)\n\n# Start training\ntrainer.train()\n\n# Save final model and tokenizer\nmodel.save_pretrained(\"./araT5-qg-final\")\ntokenizer.save_pretrained(\"./araT5-qg-final\")\n\nprint(\"✅ Training complete! Model and tokenizer saved to './araT5-qg-final'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:24:16.607334Z","iopub.execute_input":"2025-05-02T03:24:16.607623Z","iopub.status.idle":"2025-05-02T07:02:55.274252Z","shell.execute_reply.started":"2025-05-02T03:24:16.607602Z","shell.execute_reply":"2025-05-02T07:02:55.273362Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_73/540974889.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7884' max='7884' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7884/7884 3:38:30, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.486000</td>\n      <td>0.391099</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.453800</td>\n      <td>0.377061</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.445300</td>\n      <td>0.373185</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Training complete! Model and tokenizer saved to 'https://github.com/nadirfartas/araT5-qg-final'\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import torch\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"GPU name:\", torch.cuda.get_device_name(0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:20:50.698766Z","iopub.status.idle":"2025-05-02T03:20:50.699064Z","shell.execute_reply.started":"2025-05-02T03:20:50.698917Z","shell.execute_reply":"2025-05-02T03:20:50.698929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}